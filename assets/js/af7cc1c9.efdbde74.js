"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2919],{3905:function(e,t,n){n.r(t),n.d(t,{MDXContext:function(){return m},MDXProvider:function(){return s},mdx:function(){return f},useMDXComponents:function(){return c},withMDXComponents:function(){return p}});var r=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(){return a=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(e[r]=n[r])}return e},a.apply(this,arguments)}function d(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?d(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):d(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var m=r.createContext({}),p=function(e){return function(t){var n=c(t.components);return r.createElement(e,a({},t,{components:n}))}},c=function(e){var t=r.useContext(m),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},s=function(e){var t=c(e.components);return r.createElement(m.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},x=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,d=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),p=c(n),s=i,x=p["".concat(d,".").concat(s)]||p[s]||u[s]||a;return n?r.createElement(x,o(o({ref:t},m),{},{components:n})):r.createElement(x,o({ref:t},m))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,d=new Array(a);d[0]=x;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:i,d[1]=o;for(var m=2;m<a;m++)d[m]=n[m];return r.createElement.apply(null,d)}return r.createElement.apply(null,n)}x.displayName="MDXCreateElement"},40718:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return o},contentTitle:function(){return l},metadata:function(){return m},toc:function(){return p},default:function(){return s}});var r=n(83117),i=n(80102),a=(n(67294),n(3905)),d=["components"],o={id:"text_wordpiecetokenizer.wordpiecetokenizer",title:"Class: WordPieceTokenizer",sidebar_label:"WordPieceTokenizer",custom_edit_url:null},l=void 0,m={unversionedId:"api/core/classes/text_wordpiecetokenizer.wordpiecetokenizer",id:"version-0.2.0/api/core/classes/text_wordpiecetokenizer.wordpiecetokenizer",title:"Class: WordPieceTokenizer",description:"text/WordpieceTokenizer.WordPieceTokenizer",source:"@site/versioned_docs/version-0.2.0/api/core/classes/text_wordpiecetokenizer.wordpiecetokenizer.md",sourceDirName:"api/core/classes",slug:"/api/core/classes/text_wordpiecetokenizer.wordpiecetokenizer",permalink:"/live/docs/api/core/classes/text_wordpiecetokenizer.wordpiecetokenizer",editUrl:null,tags:[],version:"0.2.0",frontMatter:{id:"text_wordpiecetokenizer.wordpiecetokenizer",title:"Class: WordPieceTokenizer",sidebar_label:"WordPieceTokenizer",custom_edit_url:null},sidebar:"api",previous:{title:"BasicTokenizer",permalink:"/live/docs/api/core/classes/text_basictokenizer.basictokenizer"},next:{title:"Audio",permalink:"/live/docs/api/core/interfaces/audio_audiomodule.audio"}},p=[{value:"Constructors",id:"constructors",children:[{value:"constructor",id:"constructor",children:[{value:"Parameters",id:"parameters",children:[],level:4},{value:"Defined in",id:"defined-in",children:[],level:4}],level:3}],level:2},{value:"Methods",id:"methods",children:[{value:"decode",id:"decode",children:[{value:"Parameters",id:"parameters-1",children:[],level:4},{value:"Returns",id:"returns",children:[],level:4},{value:"Defined in",id:"defined-in-1",children:[],level:4}],level:3},{value:"encode",id:"encode",children:[{value:"Parameters",id:"parameters-2",children:[],level:4},{value:"Returns",id:"returns-1",children:[],level:4},{value:"Defined in",id:"defined-in-2",children:[],level:4}],level:3},{value:"tokenize",id:"tokenize",children:[{value:"Parameters",id:"parameters-3",children:[],level:4},{value:"Returns",id:"returns-2",children:[],level:4},{value:"Defined in",id:"defined-in-3",children:[],level:4}],level:3}],level:2}],c={toc:p};function s(e){var t=e.components,n=(0,i.Z)(e,d);return(0,a.mdx)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.mdx)("p",null,(0,a.mdx)("a",{parentName:"p",href:"/live/docs/api/core/modules/text_wordpiecetokenizer"},"text/WordpieceTokenizer"),".WordPieceTokenizer"),(0,a.mdx)("h2",{id:"constructors"},"Constructors"),(0,a.mdx)("h3",{id:"constructor"},"constructor"),(0,a.mdx)("p",null,"\u2022 ",(0,a.mdx)("strong",{parentName:"p"},"new WordPieceTokenizer"),"(",(0,a.mdx)("inlineCode",{parentName:"p"},"config"),")"),(0,a.mdx)("p",null,"Construct a tokenizer with a WordPieceTokenizer object."),(0,a.mdx)("h4",{id:"parameters"},"Parameters"),(0,a.mdx)("table",null,(0,a.mdx)("thead",{parentName:"table"},(0,a.mdx)("tr",{parentName:"thead"},(0,a.mdx)("th",{parentName:"tr",align:"left"},"Name"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Type"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Description"))),(0,a.mdx)("tbody",{parentName:"table"},(0,a.mdx)("tr",{parentName:"tbody"},(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("inlineCode",{parentName:"td"},"config")),(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("a",{parentName:"td",href:"/live/docs/api/core/modules/text_wordpiecetokenizer#wordpiecetokenizerconfig"},"WordPieceTokenizerConfig")),(0,a.mdx)("td",{parentName:"tr",align:"left"},"a tokenizer configuration object that specify the vocabulary and special tokens, etc.")))),(0,a.mdx)("h4",{id:"defined-in"},"Defined in"),(0,a.mdx)("p",null,"text/WordpieceTokenizer.ts:26"),(0,a.mdx)("h2",{id:"methods"},"Methods"),(0,a.mdx)("h3",{id:"decode"},"decode"),(0,a.mdx)("p",null,"\u25b8 ",(0,a.mdx)("strong",{parentName:"p"},"decode"),"(",(0,a.mdx)("inlineCode",{parentName:"p"},"tokenIds"),"): ",(0,a.mdx)("inlineCode",{parentName:"p"},"string")),(0,a.mdx)("p",null,"Decode an array of tokenIds to a string using the vocabulary"),(0,a.mdx)("h4",{id:"parameters-1"},"Parameters"),(0,a.mdx)("table",null,(0,a.mdx)("thead",{parentName:"table"},(0,a.mdx)("tr",{parentName:"thead"},(0,a.mdx)("th",{parentName:"tr",align:"left"},"Name"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Type"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Description"))),(0,a.mdx)("tbody",{parentName:"table"},(0,a.mdx)("tr",{parentName:"tbody"},(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("inlineCode",{parentName:"td"},"tokenIds")),(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("inlineCode",{parentName:"td"},"number"),"[]"),(0,a.mdx)("td",{parentName:"tr",align:"left"},"an array of tokenIds derived from the output of model")))),(0,a.mdx)("h4",{id:"returns"},"Returns"),(0,a.mdx)("p",null,(0,a.mdx)("inlineCode",{parentName:"p"},"string")),(0,a.mdx)("p",null,"a string decoded from the output of the model"),(0,a.mdx)("h4",{id:"defined-in-1"},"Defined in"),(0,a.mdx)("p",null,"text/WordpieceTokenizer.ts:143"),(0,a.mdx)("hr",null),(0,a.mdx)("h3",{id:"encode"},"encode"),(0,a.mdx)("p",null,"\u25b8 ",(0,a.mdx)("strong",{parentName:"p"},"encode"),"(",(0,a.mdx)("inlineCode",{parentName:"p"},"text"),"): ",(0,a.mdx)("inlineCode",{parentName:"p"},"number"),"[]"),(0,a.mdx)("p",null,"Encode the raw input to a NLP model to an array of number, which is tensorizable."),(0,a.mdx)("h4",{id:"parameters-2"},"Parameters"),(0,a.mdx)("table",null,(0,a.mdx)("thead",{parentName:"table"},(0,a.mdx)("tr",{parentName:"thead"},(0,a.mdx)("th",{parentName:"tr",align:"left"},"Name"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Type"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Description"))),(0,a.mdx)("tbody",{parentName:"table"},(0,a.mdx)("tr",{parentName:"tbody"},(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("inlineCode",{parentName:"td"},"text")),(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("inlineCode",{parentName:"td"},"string")),(0,a.mdx)("td",{parentName:"tr",align:"left"},"The raw input of the model")))),(0,a.mdx)("h4",{id:"returns-1"},"Returns"),(0,a.mdx)("p",null,(0,a.mdx)("inlineCode",{parentName:"p"},"number"),"[]"),(0,a.mdx)("p",null,"An array of number, which can then be used to create a tensor as model input with the torch.tensor API"),(0,a.mdx)("h4",{id:"defined-in-2"},"Defined in"),(0,a.mdx)("p",null,"text/WordpieceTokenizer.ts:132"),(0,a.mdx)("hr",null),(0,a.mdx)("h3",{id:"tokenize"},"tokenize"),(0,a.mdx)("p",null,"\u25b8 ",(0,a.mdx)("strong",{parentName:"p"},"tokenize"),"(",(0,a.mdx)("inlineCode",{parentName:"p"},"text"),"): ",(0,a.mdx)("inlineCode",{parentName:"p"},"string"),"[]"),(0,a.mdx)("p",null,"Tokenizes a piece of text into its word pieces.\nThis uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary."),(0,a.mdx)("h4",{id:"parameters-3"},"Parameters"),(0,a.mdx)("table",null,(0,a.mdx)("thead",{parentName:"table"},(0,a.mdx)("tr",{parentName:"thead"},(0,a.mdx)("th",{parentName:"tr",align:"left"},"Name"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Type"),(0,a.mdx)("th",{parentName:"tr",align:"left"},"Description"))),(0,a.mdx)("tbody",{parentName:"table"},(0,a.mdx)("tr",{parentName:"tbody"},(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("inlineCode",{parentName:"td"},"text")),(0,a.mdx)("td",{parentName:"tr",align:"left"},(0,a.mdx)("inlineCode",{parentName:"td"},"string")),(0,a.mdx)("td",{parentName:"tr",align:"left"},"the raw input of the model")))),(0,a.mdx)("h4",{id:"returns-2"},"Returns"),(0,a.mdx)("p",null,(0,a.mdx)("inlineCode",{parentName:"p"},"string"),"[]"),(0,a.mdx)("p",null,"an array of tokens in vocabulary representing the input text."),(0,a.mdx)("h4",{id:"defined-in-3"},"Defined in"),(0,a.mdx)("p",null,"text/WordpieceTokenizer.ts:71"))}s.isMDXComponent=!0}}]);